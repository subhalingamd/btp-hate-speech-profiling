{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subhalingamd/btp-hate-speech-profiling/blob/btp-final/our_model/our_model_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_ZSCye3qXAm"
      },
      "outputs": [],
      "source": [
        "ROOT_PATH = '/content/drive/MyDrive/btp'\n",
        "\n",
        "def _path(path):\n",
        "  return f'{ROOT_PATH}/{path}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22glthmL13fd"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qn1sYmPDtjHs"
      },
      "outputs": [],
      "source": [
        "# !wget https://nlp.stanford.edu/data/glove.twitter.27B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRsQbAaaIhL3"
      },
      "outputs": [],
      "source": [
        "# !pip install -q pytorch-lightning \n",
        "# !pip install -q bpemb  --no-deps\n",
        "# !pip install -q sentencepiece\n",
        "# !pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KNzaa4YIrzm"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import DataLoader, Dataset\n",
        "# from transformers import get_scheduler, AdamW\n",
        "\n",
        "# from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "\n",
        "# import torchtext\n",
        "# from torchtext import datasets, vocab\n",
        "# from torchtext.legacy import data as textdata\n",
        "## ???from torchtext.vocab import GloVe\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# import pytorch_lightning as pl\n",
        "# from pytorch_lightning import Trainer\n",
        "# from pytorch_lightning import LightningModule\n",
        "# from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "# from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
        "\n",
        "# from bpemb import BPEmb\n",
        "\n",
        "import random\n",
        "import os\n",
        "\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "import pickle\n",
        "\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# from pytorch_lightning.utilities.seed import seed_everything\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sklearn.linear_model import LogisticRegression as LR\n",
        "from sklearn.svm import SVC as SVM\n",
        "from sklearn.ensemble import RandomForestClassifier as RF\n",
        "from xgboost import XGBClassifier as XGB\n",
        "import lightgbm as lgb\n",
        "from sklearn.naive_bayes import MultinomialNB\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(['punkt', 'wordnet', 'stopwords'])\n",
        "nltk.download(['averaged_perceptron_tagger', 'universal_tagset'])\n",
        "nltk.download('all')\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.tag import pos_tag,map_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "pstem = PorterStemmer()\n",
        "lem = WordNetLemmatizer()\n",
        "stop_words = stopwords.words('english')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO1mtDrDA5eJ",
        "outputId": "d94a2591-7871-4312-8134-2d113723b503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/extended_omw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw-1.4.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2021.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet31.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQJy2G887MOY"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlVstp8w7o1B"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkS_lAuUYg0p"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed=42):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    # torch.manual_seed(seed)\n",
        "    # torch.cuda.manual_seed(seed)\n",
        "    # torch.cuda.manual_seed_all(seed)\n",
        "    # torch.backends.cudnn.deterministic = True\n",
        "    # torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ThiGkEN1-n5"
      },
      "source": [
        "# Args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKgohaqssSkB",
        "outputId": "4e70f78e-0083-43c7-fe18-700005fc07a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(batch_size=4, data_test='/content/drive/MyDrive/btp/inputs/data_pan21_test_en.tsv', data_train='/content/drive/MyDrive/btp/inputs/data_pan21_train_en.tsv', dropout=0.5, embedding='glove-twitter-25', epochs=25, gpu=1, lr=0.001, lr_decay=0.0005, num_rnn_layers=1, rnn_hidden_dim=8)\n"
          ]
        }
      ],
      "source": [
        "def parse_args():\n",
        "\tparser = ArgumentParser(description='Word Meaning Comparison')\n",
        "\tparser.add_argument('--data_train', type=str, default=_path('inputs/data_pan21_train_en.tsv'))\n",
        "\tparser.add_argument('--data_test', type=str, default=_path('inputs/data_pan21_test_en.tsv'))\n",
        "\tparser.add_argument('--embedding', type=str, default=\"glove-twitter-25\", \n",
        "\t                    choices=[\"glove-twitter-200\", \"glove-twitter-100\", \"glove-twitter-50\", \"glove-twitter-25\"])\n",
        "\t\n",
        "\tparser.add_argument('--gpu', type=int, default=1)\n",
        "\tparser.add_argument('--batch_size', type=int, default=4)\n",
        "\tparser.add_argument('--epochs', type=int, default=25)\n",
        "\tparser.add_argument('--lr', type=float, default=0.001)\n",
        "\tparser.add_argument('--lr_decay', type=float, default=0.0005)\t# removed\n",
        "\n",
        "\tparser.add_argument('--rnn_hidden_dim', type=int, default=8)\n",
        "\tparser.add_argument('--num_rnn_layers', type=int, default=1)\n",
        "\tparser.add_argument('--dropout', type=float, default=0.5)\n",
        "\n",
        "\treturn parser.parse_known_args()[0]\n",
        "\n",
        "\n",
        "args = parse_args()\n",
        "print(args)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Lcdc5wmGqqkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgYREd_6Dwz_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aae3d13-1d17-4a6b-d658-93b4118aaf4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "word_embedding = api.load(args.embedding)\n",
        "args.embedding_dim = word_embedding['word'].shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args.embedding_dim = word_embedding['word'].shape[0]"
      ],
      "metadata": {
        "id": "LUzxg57Rqrff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTMusoX_2CrJ"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJzwV-5bSxqV"
      },
      "outputs": [],
      "source": [
        "def remove_punctuations(text):\n",
        "    text = text.replace('##emoji##', '').replace('#url#', '').replace('#user#', '').replace('##rt##', '')#.replace('#hashtag#','')\n",
        "    exclist = string.punctuation #+ string.digits\n",
        "    table = str.maketrans(exclist, ' '*len(exclist))\n",
        "    return ' '.join(text.translate(table).split())\n",
        "\n",
        "def clean_df(df):\n",
        "  for col in [x for x in df.columns if x != 'label']:\n",
        "    df[col] = df[col].apply(remove_punctuations)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G98VtxIHJOLf"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(args.data_train, sep='\\t')\n",
        "df_test = pd.read_csv(args.data_test, sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df_train, df_val = train_test_split(df_train_,\n",
        "#                                     stratify=df_train_['label'].tolist(), \n",
        "#                                     test_size=0.1,\n",
        "#                                     shuffle=True,\n",
        "#                                     random_state=42,\n",
        "#                                     )"
      ],
      "metadata": {
        "id": "ozWoNUD5PaSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWtRoTay2Oll",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 803
        },
        "outputId": "5b3e0c84-84ff-4f96-9e06-b4adf57b86ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             tweet_1  \\\n",
              "0   ##rt## funny how 15 days to slow the spread t...   \n",
              "1  if you havent tried parler yet please give it ...   \n",
              "2   ##rt## disagreement with our scholarship is n...   \n",
              "3   #user# #user# tell people what to do hmm that...   \n",
              "4   #user# none at all just the ability to lie an...   \n",
              "\n",
              "                                             tweet_2  \\\n",
              "0   ##rt## why did minneapolis just give george f...   \n",
              "1   ##emoji## ##emoji## my president ##emoji## ##...   \n",
              "2   ##rt## today is the 52nd day of the biden adm...   \n",
              "3   ##rt## #user# i guarantee the signature rejec...   \n",
              "4   #user# actually most seem to be communists wh...   \n",
              "\n",
              "                                             tweet_3  \\\n",
              "0   ##rt## to be fair he has done a lot of underc...   \n",
              "1      #user# my president ##emoji## ##emoji## bump    \n",
              "2   ##rt## the cartels control the us-mexico bord...   \n",
              "3   ##rt## its easy to be woke when youve never g...   \n",
              "4   #user# spot on theyd both bare their arses on...   \n",
              "\n",
              "                                             tweet_4  \\\n",
              "0   ##rt## president trump got us the #hashtag# v...   \n",
              "1   #user# i did election night newsmax for me ##...   \n",
              "2   ##rt## and again clearly the federal courthou...   \n",
              "3   #user# imagine how many handguns can $14m get...   \n",
              "4   #user# with your media and government the ver...   \n",
              "\n",
              "                                             tweet_5  \\\n",
              "0   ##rt## is the case against former officer der...   \n",
              "1  i ##emoji## imagine that ##emoji## confidentia...   \n",
              "2   #user# the california mom who did the same th...   \n",
              "3   #user# victims of black crimes too just like ...   \n",
              "4   #user# if there is a god god willing he will ...   \n",
              "\n",
              "                                             tweet_6  \\\n",
              "0   ##rt## the fbi cia and military now feel tota...   \n",
              "1   ##emoji## ##emoji## lets show usa today who w...   \n",
              "2  yes #hashtag# and youre a hateful bigot if you...   \n",
              "3   #user# #user# trump is a fucking fail preside...   \n",
              "4   #user# #user# surely a careless injection is ...   \n",
              "\n",
              "                                             tweet_7  \\\n",
              "0   ##rt## biden bashed for taking all credit for...   \n",
              "1   #user# ##emoji## ##emoji## get well soon sir ...   \n",
              "2   #user# #user# #user# blew her claim out of th...   \n",
              "3  i just edited this headline story on #hashtag#...   \n",
              "4   ##rt## im shocked i tell you shocked ##emoji#...   \n",
              "\n",
              "                                             tweet_8  \\\n",
              "0   ##rt## bidens disappointing prime-time speech...   \n",
              "1  we are all #hashtag# for you and our beautiful...   \n",
              "2   #user# #user# #user# #user# ooh my favorite d...   \n",
              "3                             harvard = trash #url#    \n",
              "4   #user# with an appointment taking 3 weeks i d...   \n",
              "\n",
              "                                             tweet_9  \\\n",
              "0   ##rt## cuomo shouldnt be worried about resign...   \n",
              "1   ##rt## #user# do you support our potus (pleas...   \n",
              "2   ##rt## woke christian asks jesus to return al...   \n",
              "3   ##rt## heres to all the dads doing their best...   \n",
              "4   #user# i saw a tv advert with a white person ...   \n",
              "\n",
              "                                            tweet_10  ...  \\\n",
              "0   ##rt## breaking family of george floyd settle...  ...   \n",
              "1              #user# law and order thank you potus   ...   \n",
              "2              ##rt## depends on the training #url#   ...   \n",
              "3   ##rt## welcome to california the politicians ...  ...   \n",
              "4   #user# wow not only are you a fantastic artis...  ...   \n",
              "\n",
              "                                           tweet_192  \\\n",
              "0   ##rt## why isnt cuomos deadly coronavirus cov...   \n",
              "1  anybody would be nuts to vote for this guy ##e...   \n",
              "2   ##rt## this is a l you posted cringe you just...   \n",
              "3   ##rt## i copied this from parler it is so goo...   \n",
              "4   #user# how is the press racist the better que...   \n",
              "\n",
              "                                           tweet_193  \\\n",
              "0   ##rt## can we get more information about this...   \n",
              "1   ##rt## rt if its time institute nationwide vo...   \n",
              "2   ##rt## remember my wise friend we caught up t...   \n",
              "3   #user# #user# where is the evidence that he i...   \n",
              "4   #user# now i think the royals have done untol...   \n",
              "\n",
              "                                           tweet_194  \\\n",
              "0   ##rt## raise your hand if you wont be wearing...   \n",
              "1  more ppl need to hear this doctor tell it like...   \n",
              "2   ##rt## correct judge (correctly noted that am...   \n",
              "3      whats hunter bidens next job after dad loses    \n",
              "4   #user# what utter bullshit what is the source...   \n",
              "\n",
              "                                           tweet_195  \\\n",
              "0   ##rt## which is precisely why the media did w...   \n",
              "1                   ##emoji## truth ##emoji## #url#    \n",
              "2   ##rt## joe biden opened himself up to questio...   \n",
              "3                             #user# vote for trump    \n",
              "4                #user# no the uk but god knows how    \n",
              "\n",
              "                                           tweet_196  \\\n",
              "0   ##rt## poll one in six biden voters would hav...   \n",
              "1  dream on honey ##emoji## ##emoji## ##emoji## #...   \n",
              "2   ##rt## warning this thread may make you sick ...   \n",
              "3   ##rt## air force veteran aja smith hopes to b...   \n",
              "4   #user# this has to be one of the best pranks ...   \n",
              "\n",
              "                                           tweet_197  \\\n",
              "0             ##rt## i just spit out my water #url#    \n",
              "1  i so want to see her and bill face justice #url#    \n",
              "2  youre never gonna unsee that you can thank me ...   \n",
              "3  i love hunter biden he is fucking up his dads ...   \n",
              "4   #user# take yours out of your mates hand and ...   \n",
              "\n",
              "                                           tweet_198  \\\n",
              "0   ##rt## because its cracked pipes he would be ...   \n",
              "1                                  i hope so@ #url#    \n",
              "2  but why is nicole kidman dressed as a fruit ro...   \n",
              "3                         who got the worse brother    \n",
              "4                #user# chunk is a better word imho    \n",
              "\n",
              "                                           tweet_199  \\\n",
              "0   ##rt## donald trump is back on the record – a...   \n",
              "1   ##rt## deplorables mount up ##emoji## ##emoji...   \n",
              "2   #user# according to the theory on allergies s...   \n",
              "3   ##rt## suspending white house officials twitt...   \n",
              "4   #user# fuck off you moron please lets not be ...   \n",
              "\n",
              "                                           tweet_200 label  \n",
              "0   ##rt## trump showed republicans how to help m...     0  \n",
              "1             #user# im with ya #hashtag# #hashtag#      0  \n",
              "2        ##rt## the new total is 1927000 signatures      0  \n",
              "3   #user# #user# all the republican majority are...     0  \n",
              "4   #user# just checked your bio 40 followers eve...     0  \n",
              "\n",
              "[5 rows x 201 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fd4c3293-98ae-497b-b6c9-22b92871b6d0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_1</th>\n",
              "      <th>tweet_2</th>\n",
              "      <th>tweet_3</th>\n",
              "      <th>tweet_4</th>\n",
              "      <th>tweet_5</th>\n",
              "      <th>tweet_6</th>\n",
              "      <th>tweet_7</th>\n",
              "      <th>tweet_8</th>\n",
              "      <th>tweet_9</th>\n",
              "      <th>tweet_10</th>\n",
              "      <th>...</th>\n",
              "      <th>tweet_192</th>\n",
              "      <th>tweet_193</th>\n",
              "      <th>tweet_194</th>\n",
              "      <th>tweet_195</th>\n",
              "      <th>tweet_196</th>\n",
              "      <th>tweet_197</th>\n",
              "      <th>tweet_198</th>\n",
              "      <th>tweet_199</th>\n",
              "      <th>tweet_200</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>##rt## funny how 15 days to slow the spread t...</td>\n",
              "      <td>##rt## why did minneapolis just give george f...</td>\n",
              "      <td>##rt## to be fair he has done a lot of underc...</td>\n",
              "      <td>##rt## president trump got us the #hashtag# v...</td>\n",
              "      <td>##rt## is the case against former officer der...</td>\n",
              "      <td>##rt## the fbi cia and military now feel tota...</td>\n",
              "      <td>##rt## biden bashed for taking all credit for...</td>\n",
              "      <td>##rt## bidens disappointing prime-time speech...</td>\n",
              "      <td>##rt## cuomo shouldnt be worried about resign...</td>\n",
              "      <td>##rt## breaking family of george floyd settle...</td>\n",
              "      <td>...</td>\n",
              "      <td>##rt## why isnt cuomos deadly coronavirus cov...</td>\n",
              "      <td>##rt## can we get more information about this...</td>\n",
              "      <td>##rt## raise your hand if you wont be wearing...</td>\n",
              "      <td>##rt## which is precisely why the media did w...</td>\n",
              "      <td>##rt## poll one in six biden voters would hav...</td>\n",
              "      <td>##rt## i just spit out my water #url#</td>\n",
              "      <td>##rt## because its cracked pipes he would be ...</td>\n",
              "      <td>##rt## donald trump is back on the record – a...</td>\n",
              "      <td>##rt## trump showed republicans how to help m...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>if you havent tried parler yet please give it ...</td>\n",
              "      <td>##emoji## ##emoji## my president ##emoji## ##...</td>\n",
              "      <td>#user# my president ##emoji## ##emoji## bump</td>\n",
              "      <td>#user# i did election night newsmax for me ##...</td>\n",
              "      <td>i ##emoji## imagine that ##emoji## confidentia...</td>\n",
              "      <td>##emoji## ##emoji## lets show usa today who w...</td>\n",
              "      <td>#user# ##emoji## ##emoji## get well soon sir ...</td>\n",
              "      <td>we are all #hashtag# for you and our beautiful...</td>\n",
              "      <td>##rt## #user# do you support our potus (pleas...</td>\n",
              "      <td>#user# law and order thank you potus</td>\n",
              "      <td>...</td>\n",
              "      <td>anybody would be nuts to vote for this guy ##e...</td>\n",
              "      <td>##rt## rt if its time institute nationwide vo...</td>\n",
              "      <td>more ppl need to hear this doctor tell it like...</td>\n",
              "      <td>##emoji## truth ##emoji## #url#</td>\n",
              "      <td>dream on honey ##emoji## ##emoji## ##emoji## #...</td>\n",
              "      <td>i so want to see her and bill face justice #url#</td>\n",
              "      <td>i hope so@ #url#</td>\n",
              "      <td>##rt## deplorables mount up ##emoji## ##emoji...</td>\n",
              "      <td>#user# im with ya #hashtag# #hashtag#</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>##rt## disagreement with our scholarship is n...</td>\n",
              "      <td>##rt## today is the 52nd day of the biden adm...</td>\n",
              "      <td>##rt## the cartels control the us-mexico bord...</td>\n",
              "      <td>##rt## and again clearly the federal courthou...</td>\n",
              "      <td>#user# the california mom who did the same th...</td>\n",
              "      <td>yes #hashtag# and youre a hateful bigot if you...</td>\n",
              "      <td>#user# #user# #user# blew her claim out of th...</td>\n",
              "      <td>#user# #user# #user# #user# ooh my favorite d...</td>\n",
              "      <td>##rt## woke christian asks jesus to return al...</td>\n",
              "      <td>##rt## depends on the training #url#</td>\n",
              "      <td>...</td>\n",
              "      <td>##rt## this is a l you posted cringe you just...</td>\n",
              "      <td>##rt## remember my wise friend we caught up t...</td>\n",
              "      <td>##rt## correct judge (correctly noted that am...</td>\n",
              "      <td>##rt## joe biden opened himself up to questio...</td>\n",
              "      <td>##rt## warning this thread may make you sick ...</td>\n",
              "      <td>youre never gonna unsee that you can thank me ...</td>\n",
              "      <td>but why is nicole kidman dressed as a fruit ro...</td>\n",
              "      <td>#user# according to the theory on allergies s...</td>\n",
              "      <td>##rt## the new total is 1927000 signatures</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>#user# #user# tell people what to do hmm that...</td>\n",
              "      <td>##rt## #user# i guarantee the signature rejec...</td>\n",
              "      <td>##rt## its easy to be woke when youve never g...</td>\n",
              "      <td>#user# imagine how many handguns can $14m get...</td>\n",
              "      <td>#user# victims of black crimes too just like ...</td>\n",
              "      <td>#user# #user# trump is a fucking fail preside...</td>\n",
              "      <td>i just edited this headline story on #hashtag#...</td>\n",
              "      <td>harvard = trash #url#</td>\n",
              "      <td>##rt## heres to all the dads doing their best...</td>\n",
              "      <td>##rt## welcome to california the politicians ...</td>\n",
              "      <td>...</td>\n",
              "      <td>##rt## i copied this from parler it is so goo...</td>\n",
              "      <td>#user# #user# where is the evidence that he i...</td>\n",
              "      <td>whats hunter bidens next job after dad loses</td>\n",
              "      <td>#user# vote for trump</td>\n",
              "      <td>##rt## air force veteran aja smith hopes to b...</td>\n",
              "      <td>i love hunter biden he is fucking up his dads ...</td>\n",
              "      <td>who got the worse brother</td>\n",
              "      <td>##rt## suspending white house officials twitt...</td>\n",
              "      <td>#user# #user# all the republican majority are...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>#user# none at all just the ability to lie an...</td>\n",
              "      <td>#user# actually most seem to be communists wh...</td>\n",
              "      <td>#user# spot on theyd both bare their arses on...</td>\n",
              "      <td>#user# with your media and government the ver...</td>\n",
              "      <td>#user# if there is a god god willing he will ...</td>\n",
              "      <td>#user# #user# surely a careless injection is ...</td>\n",
              "      <td>##rt## im shocked i tell you shocked ##emoji#...</td>\n",
              "      <td>#user# with an appointment taking 3 weeks i d...</td>\n",
              "      <td>#user# i saw a tv advert with a white person ...</td>\n",
              "      <td>#user# wow not only are you a fantastic artis...</td>\n",
              "      <td>...</td>\n",
              "      <td>#user# how is the press racist the better que...</td>\n",
              "      <td>#user# now i think the royals have done untol...</td>\n",
              "      <td>#user# what utter bullshit what is the source...</td>\n",
              "      <td>#user# no the uk but god knows how</td>\n",
              "      <td>#user# this has to be one of the best pranks ...</td>\n",
              "      <td>#user# take yours out of your mates hand and ...</td>\n",
              "      <td>#user# chunk is a better word imho</td>\n",
              "      <td>#user# fuck off you moron please lets not be ...</td>\n",
              "      <td>#user# just checked your bio 40 followers eve...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 201 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fd4c3293-98ae-497b-b6c9-22b92871b6d0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fd4c3293-98ae-497b-b6c9-22b92871b6d0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fd4c3293-98ae-497b-b6c9-22b92871b6d0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgHzmhvG10jn"
      },
      "outputs": [],
      "source": [
        "df_train = clean_df(df_train)\n",
        "df_test = clean_df(df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bX_Ev0mRTgyx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "outputId": "64aedd1f-749f-47b9-8494-a45fde141c05"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             tweet_1  \\\n",
              "0  funny how 15 days to slow the spread turned in...   \n",
              "1  if you havent tried parler yet please give it ...   \n",
              "2  disagreement with our scholarship is not possi...   \n",
              "3  tell people what to do hmm that sounds communi...   \n",
              "4  none at all just the ability to lie and adore ...   \n",
              "\n",
              "                                             tweet_2  \\\n",
              "0  why did minneapolis just give george floyds fa...   \n",
              "1                                       my president   \n",
              "2  today is the 52nd day of the biden administrat...   \n",
              "3  i guarantee the signature reject rate will be ...   \n",
              "4  actually most seem to be communists which as t...   \n",
              "\n",
              "                                             tweet_3  \\\n",
              "0  to be fair he has done a lot of undercover wor...   \n",
              "1                                  my president bump   \n",
              "2  the cartels control the us mexico border on ru...   \n",
              "3  its easy to be woke when youve never gotten ju...   \n",
              "4  spot on theyd both bare their arses on holywoo...   \n",
              "\n",
              "                                             tweet_4  \\\n",
              "0  president trump got us the hashtag vaccine pre...   \n",
              "1                i did election night newsmax for me   \n",
              "2  and again clearly the federal courthouse in po...   \n",
              "3  imagine how many handguns can 14m get for asia...   \n",
              "4  with your media and government the very talk o...   \n",
              "\n",
              "                                             tweet_5  \\\n",
              "0  is the case against former officer derek chauv...   \n",
              "1  i imagine that confidential documents obtained...   \n",
              "2  the california mom who did the same thing is o...   \n",
              "3  victims of black crimes too just like the othe...   \n",
              "4  if there is a god god willing he will do no su...   \n",
              "\n",
              "                                             tweet_6  \\\n",
              "0  the fbi cia and military now feel totally comf...   \n",
              "1             lets show usa today who won the debate   \n",
              "2  yes hashtag and youre a hateful bigot if you t...   \n",
              "3  trump is a fucking fail president he placed th...   \n",
              "4  surely a careless injection is not going to be...   \n",
              "\n",
              "                                             tweet_7  \\\n",
              "0  biden bashed for taking all credit for covid 1...   \n",
              "1                      get well soon sir we love you   \n",
              "2  blew her claim out of the water anyway it was ...   \n",
              "3  i just edited this headline story on hashtag y...   \n",
              "4                      im shocked i tell you shocked   \n",
              "\n",
              "                                             tweet_8  \\\n",
              "0  bidens disappointing prime time speech was a c...   \n",
              "1  we are all hashtag for you and our beautiful h...   \n",
              "2                            ooh my favorite dessert   \n",
              "3                                      harvard trash   \n",
              "4  with an appointment taking 3 weeks i doubt he ...   \n",
              "\n",
              "                                             tweet_9  \\\n",
              "0  cuomo shouldnt be worried about resigning he s...   \n",
              "1  do you support our potus please help by retwee...   \n",
              "2  woke christian asks jesus to return all the sh...   \n",
              "3             heres to all the dads doing their best   \n",
              "4  i saw a tv advert with a white person in it th...   \n",
              "\n",
              "                                            tweet_10  ...  \\\n",
              "0  breaking family of george floyd settle wrongfu...  ...   \n",
              "1                      law and order thank you potus  ...   \n",
              "2                            depends on the training  ...   \n",
              "3  welcome to california the politicians that run...  ...   \n",
              "4  wow not only are you a fantastic artist youre ...  ...   \n",
              "\n",
              "                                           tweet_192  \\\n",
              "0  why isnt cuomos deadly coronavirus cover up th...   \n",
              "1         anybody would be nuts to vote for this guy   \n",
              "2  this is a l you posted cringe you just admitte...   \n",
              "3  i copied this from parler it is so good and th...   \n",
              "4  how is the press racist the better question is...   \n",
              "\n",
              "                                           tweet_193  \\\n",
              "0  can we get more information about this speaker...   \n",
              "1  rt if its time institute nationwide voter id f...   \n",
              "2         remember my wise friend we caught up today   \n",
              "3  where is the evidence that he is a white supre...   \n",
              "4  now i think the royals have done untold damage...   \n",
              "\n",
              "                                           tweet_194  \\\n",
              "0  raise your hand if you wont be wearing a mask ...   \n",
              "1  more ppl need to hear this doctor tell it like...   \n",
              "2  correct judge correctly noted that am referenc...   \n",
              "3       whats hunter bidens next job after dad loses   \n",
              "4  what utter bullshit what is the source of your...   \n",
              "\n",
              "                                           tweet_195  \\\n",
              "0  which is precisely why the media did whatever ...   \n",
              "1                                              truth   \n",
              "2  joe biden opened himself up to questions from ...   \n",
              "3                                     vote for trump   \n",
              "4                        no the uk but god knows how   \n",
              "\n",
              "                                           tweet_196  \\\n",
              "0  poll one in six biden voters would have change...   \n",
              "1                                     dream on honey   \n",
              "2              warning this thread may make you sick   \n",
              "3  air force veteran aja smith hopes to be first ...   \n",
              "4  this has to be one of the best pranks i have e...   \n",
              "\n",
              "                                           tweet_197  \\\n",
              "0                           i just spit out my water   \n",
              "1         i so want to see her and bill face justice   \n",
              "2  youre never gonna unsee that you can thank me ...   \n",
              "3  i love hunter biden he is fucking up his dads ...   \n",
              "4    take yours out of your mates hand and get a job   \n",
              "\n",
              "                                           tweet_198  \\\n",
              "0  because its cracked pipes he would be dealing ...   \n",
              "1                                          i hope so   \n",
              "2  but why is nicole kidman dressed as a fruit ro...   \n",
              "3                          who got the worse brother   \n",
              "4                        chunk is a better word imho   \n",
              "\n",
              "                                           tweet_199  \\\n",
              "0  donald trump is back on the record – asked abo...   \n",
              "1                               deplorables mount up   \n",
              "2  according to the theory on allergies some of m...   \n",
              "3  suspending white house officials twitter accou...   \n",
              "4         fuck off you moron please lets not be rude   \n",
              "\n",
              "                                           tweet_200 label  \n",
              "0  trump showed republicans how to help minority ...     0  \n",
              "1                         im with ya hashtag hashtag     0  \n",
              "2                the new total is 1927000 signatures     0  \n",
              "3     all the republican majority areas be shut down     0  \n",
              "4  just checked your bio 40 followers everything ...     0  \n",
              "\n",
              "[5 rows x 201 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1af09f83-43c6-41cb-9027-b41b3446f0c0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_1</th>\n",
              "      <th>tweet_2</th>\n",
              "      <th>tweet_3</th>\n",
              "      <th>tweet_4</th>\n",
              "      <th>tweet_5</th>\n",
              "      <th>tweet_6</th>\n",
              "      <th>tweet_7</th>\n",
              "      <th>tweet_8</th>\n",
              "      <th>tweet_9</th>\n",
              "      <th>tweet_10</th>\n",
              "      <th>...</th>\n",
              "      <th>tweet_192</th>\n",
              "      <th>tweet_193</th>\n",
              "      <th>tweet_194</th>\n",
              "      <th>tweet_195</th>\n",
              "      <th>tweet_196</th>\n",
              "      <th>tweet_197</th>\n",
              "      <th>tweet_198</th>\n",
              "      <th>tweet_199</th>\n",
              "      <th>tweet_200</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>funny how 15 days to slow the spread turned in...</td>\n",
              "      <td>why did minneapolis just give george floyds fa...</td>\n",
              "      <td>to be fair he has done a lot of undercover wor...</td>\n",
              "      <td>president trump got us the hashtag vaccine pre...</td>\n",
              "      <td>is the case against former officer derek chauv...</td>\n",
              "      <td>the fbi cia and military now feel totally comf...</td>\n",
              "      <td>biden bashed for taking all credit for covid 1...</td>\n",
              "      <td>bidens disappointing prime time speech was a c...</td>\n",
              "      <td>cuomo shouldnt be worried about resigning he s...</td>\n",
              "      <td>breaking family of george floyd settle wrongfu...</td>\n",
              "      <td>...</td>\n",
              "      <td>why isnt cuomos deadly coronavirus cover up th...</td>\n",
              "      <td>can we get more information about this speaker...</td>\n",
              "      <td>raise your hand if you wont be wearing a mask ...</td>\n",
              "      <td>which is precisely why the media did whatever ...</td>\n",
              "      <td>poll one in six biden voters would have change...</td>\n",
              "      <td>i just spit out my water</td>\n",
              "      <td>because its cracked pipes he would be dealing ...</td>\n",
              "      <td>donald trump is back on the record – asked abo...</td>\n",
              "      <td>trump showed republicans how to help minority ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>if you havent tried parler yet please give it ...</td>\n",
              "      <td>my president</td>\n",
              "      <td>my president bump</td>\n",
              "      <td>i did election night newsmax for me</td>\n",
              "      <td>i imagine that confidential documents obtained...</td>\n",
              "      <td>lets show usa today who won the debate</td>\n",
              "      <td>get well soon sir we love you</td>\n",
              "      <td>we are all hashtag for you and our beautiful h...</td>\n",
              "      <td>do you support our potus please help by retwee...</td>\n",
              "      <td>law and order thank you potus</td>\n",
              "      <td>...</td>\n",
              "      <td>anybody would be nuts to vote for this guy</td>\n",
              "      <td>rt if its time institute nationwide voter id f...</td>\n",
              "      <td>more ppl need to hear this doctor tell it like...</td>\n",
              "      <td>truth</td>\n",
              "      <td>dream on honey</td>\n",
              "      <td>i so want to see her and bill face justice</td>\n",
              "      <td>i hope so</td>\n",
              "      <td>deplorables mount up</td>\n",
              "      <td>im with ya hashtag hashtag</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>disagreement with our scholarship is not possi...</td>\n",
              "      <td>today is the 52nd day of the biden administrat...</td>\n",
              "      <td>the cartels control the us mexico border on ru...</td>\n",
              "      <td>and again clearly the federal courthouse in po...</td>\n",
              "      <td>the california mom who did the same thing is o...</td>\n",
              "      <td>yes hashtag and youre a hateful bigot if you t...</td>\n",
              "      <td>blew her claim out of the water anyway it was ...</td>\n",
              "      <td>ooh my favorite dessert</td>\n",
              "      <td>woke christian asks jesus to return all the sh...</td>\n",
              "      <td>depends on the training</td>\n",
              "      <td>...</td>\n",
              "      <td>this is a l you posted cringe you just admitte...</td>\n",
              "      <td>remember my wise friend we caught up today</td>\n",
              "      <td>correct judge correctly noted that am referenc...</td>\n",
              "      <td>joe biden opened himself up to questions from ...</td>\n",
              "      <td>warning this thread may make you sick</td>\n",
              "      <td>youre never gonna unsee that you can thank me ...</td>\n",
              "      <td>but why is nicole kidman dressed as a fruit ro...</td>\n",
              "      <td>according to the theory on allergies some of m...</td>\n",
              "      <td>the new total is 1927000 signatures</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tell people what to do hmm that sounds communi...</td>\n",
              "      <td>i guarantee the signature reject rate will be ...</td>\n",
              "      <td>its easy to be woke when youve never gotten ju...</td>\n",
              "      <td>imagine how many handguns can 14m get for asia...</td>\n",
              "      <td>victims of black crimes too just like the othe...</td>\n",
              "      <td>trump is a fucking fail president he placed th...</td>\n",
              "      <td>i just edited this headline story on hashtag y...</td>\n",
              "      <td>harvard trash</td>\n",
              "      <td>heres to all the dads doing their best</td>\n",
              "      <td>welcome to california the politicians that run...</td>\n",
              "      <td>...</td>\n",
              "      <td>i copied this from parler it is so good and th...</td>\n",
              "      <td>where is the evidence that he is a white supre...</td>\n",
              "      <td>whats hunter bidens next job after dad loses</td>\n",
              "      <td>vote for trump</td>\n",
              "      <td>air force veteran aja smith hopes to be first ...</td>\n",
              "      <td>i love hunter biden he is fucking up his dads ...</td>\n",
              "      <td>who got the worse brother</td>\n",
              "      <td>suspending white house officials twitter accou...</td>\n",
              "      <td>all the republican majority areas be shut down</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>none at all just the ability to lie and adore ...</td>\n",
              "      <td>actually most seem to be communists which as t...</td>\n",
              "      <td>spot on theyd both bare their arses on holywoo...</td>\n",
              "      <td>with your media and government the very talk o...</td>\n",
              "      <td>if there is a god god willing he will do no su...</td>\n",
              "      <td>surely a careless injection is not going to be...</td>\n",
              "      <td>im shocked i tell you shocked</td>\n",
              "      <td>with an appointment taking 3 weeks i doubt he ...</td>\n",
              "      <td>i saw a tv advert with a white person in it th...</td>\n",
              "      <td>wow not only are you a fantastic artist youre ...</td>\n",
              "      <td>...</td>\n",
              "      <td>how is the press racist the better question is...</td>\n",
              "      <td>now i think the royals have done untold damage...</td>\n",
              "      <td>what utter bullshit what is the source of your...</td>\n",
              "      <td>no the uk but god knows how</td>\n",
              "      <td>this has to be one of the best pranks i have e...</td>\n",
              "      <td>take yours out of your mates hand and get a job</td>\n",
              "      <td>chunk is a better word imho</td>\n",
              "      <td>fuck off you moron please lets not be rude</td>\n",
              "      <td>just checked your bio 40 followers everything ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 201 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1af09f83-43c6-41cb-9027-b41b3446f0c0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1af09f83-43c6-41cb-9027-b41b3446f0c0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1af09f83-43c6-41cb-9027-b41b3446f0c0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "9DkZAageKN0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tk = nltk.TweetTokenizer()\n",
        "def process_dataset(data, embedding, pooling='mean'):\n",
        "    assert len(data) > 0, 'data must be a non-empty list'\n",
        "    unk_count, unk_emb_count, unk_swn_count = 0, 0, 0\n",
        "    def _get_swn_score(s, embedding):\n",
        "        unk_count=0\n",
        "        pos, neut, neg = np.zeros(args.embedding_dim), np.zeros(args.embedding_dim), np.zeros(args.embedding_dim)\n",
        "        norm_pos, norm_neut, norm_neg = 0, 0, 0\n",
        "        ## s = tk.tokenize(s)\n",
        "        ## tokens = [x for x in s if x not in stop_words]\n",
        "        s = ' '.join([x for x in s.split() if x not in stop_words])\n",
        "        tokens = s.split()\n",
        "        tagged_sent = pos_tag(tokens)\n",
        "        store_it = [(word, map_tag('en-ptb', 'universal', tag)) for word, tag in tagged_sent]\n",
        "        #print(\"Tagged Parts of Speech:\",store_it)\n",
        "\n",
        "        for word,tag in store_it:\n",
        "            if(tag=='NOUN'):\n",
        "                tag='n'\n",
        "            elif(tag=='VERB'):\n",
        "                tag='v'\n",
        "            elif(tag=='ADJ'):\n",
        "                tag='a'\n",
        "            elif(tag=='ADV'):\n",
        "                tag = 'r'\n",
        "            else:\n",
        "                tag='nothing'\n",
        "            \n",
        "            if tag == 'nothing':\n",
        "                unk_count += 1\n",
        "                continue\n",
        "            else:\n",
        "                # if word in stop_words:\n",
        "                #     continue\n",
        "                concat = word+'.'+tag+'.01'\n",
        "                try:\n",
        "                    sent = swn.senti_synset(concat)\n",
        "                    emb = embedding[word]\n",
        "                    pos += sent.pos_score()*emb\n",
        "                    neut += sent.obj_score()*emb\n",
        "                    neg += sent.neg_score()*emb\n",
        "                    norm_pos, norm_neut, norm_neg = norm_pos + sent.pos_score(), norm_neut + sent.obj_score(), norm_neg + sent.neg_score()\n",
        "                    #print(word,tag,':',this_word_pos,this_word_neg)\n",
        "                # except Exception as e:\n",
        "                #     wor = lem.lemmatize(word)\n",
        "                #     # if wor in stop_words:\n",
        "                #     #     continue\n",
        "                #     concat = wor+'.'+tag+'.01'\n",
        "                #     # Checking if there's a possiblity of lemmatized word be accepted into SWN corpus\n",
        "                #     try:\n",
        "                #         sent = swn.senti_synset(concat)\n",
        "                #         emb = embedding[wor]\n",
        "                #         pos += sent.pos_score()*emb\n",
        "                #         neut += sent.obj_score()*emb\n",
        "                #         neg += sent.neg_score()*emb\n",
        "                #         norm_pos, norm_neut, norm_neg = norm_pos + sent.pos_score(), norm_neut + sent.obj_score(), norm_neg + sent.neg_score()\n",
        "                #     except Exception as e:\n",
        "                #         wor = pstem.stem(word)\n",
        "                #         # if wor in stop_words:\n",
        "                #         #     continue\n",
        "                #         concat = wor+'.'+tag+'.01'\n",
        "                #         # Checking if there's a possiblity of stemmed word be accepted\n",
        "                #         try:\n",
        "                #             sent = swn.senti_synset(concat)\n",
        "                #             emb = embedding[wor]\n",
        "                #             pos += sent.pos_score()*emb\n",
        "                #             neut += sent.obj_score()*emb\n",
        "                #             neg += sent.neg_score()*emb\n",
        "                #             norm_pos, norm_neut, norm_neg = norm_pos + sent.pos_score(), norm_neut + sent.obj_score(), norm_neg + sent.neg_score()\n",
        "                #         except:\n",
        "                #             unk_count += 1\n",
        "                #             continue\n",
        "                except:\n",
        "                        unk_count += 1\n",
        "                        continue\n",
        "                            \n",
        "        # print(unk_count, len(s.split()))\n",
        "        # return (  pos/norm_pos if norm_pos != 0 else pos, \n",
        "        #           neut/norm_neut if norm_neut != 0 else neut,\n",
        "        #           neg/norm_neg if norm_neg != 0 else neg  )\n",
        "        return pos, neut, neg\n",
        "        ##return [neg]\n",
        "\n",
        "    def _get_embeddings(tweets, pooling, embedding):\n",
        "        assert pooling in (\"mean\", \"max\", \"min\", \"abs-max\")\n",
        "        \n",
        "        result = np.stack(\n",
        "            [np.concatenate(_get_swn_score(t, embedding=embedding), axis=0) \n",
        "                for t in tweets], \n",
        "            axis=0,\n",
        "            )\n",
        "        \n",
        "        # print(result.shape)\n",
        "        \n",
        "        if pooling == 'mean':\n",
        "            return np.mean(result, axis=0)\n",
        "        elif pooling == 'max':\n",
        "            return np.max(result, axis=0)\n",
        "        elif pooling == 'min':\n",
        "            return np.min(result, axis=0)\n",
        "        elif pooling == 'abs-max':\n",
        "            raise NotImplementedError\n",
        "            # return result[\n",
        "            #             torch.max(torch.abs(result), dim=0).indices, \n",
        "            #             list(range(result.size(1)))\n",
        "            #             ]\n",
        "\n",
        "\n",
        "    label_col = 'label'\n",
        "    tweet_cols = [x for x in data.columns if x!=label_col]\n",
        "    assert len(tweet_cols) == len(data.columns) - 1 == 200\n",
        "\n",
        "    _labels = data[label_col].tolist()\n",
        "    _tweets = data[tweet_cols].values.tolist()\n",
        "\n",
        "\n",
        "    data_emb, data_label = [], []\n",
        "    for label, tweets in zip(tqdm(_labels), _tweets):\n",
        "        emb = _get_embeddings(tweets, pooling=pooling, embedding=embedding)\n",
        "        # print(emb.shape)\n",
        "        assert emb.shape[0] == 3*args.embedding_dim\n",
        "        data_emb.append(emb)\n",
        "        data_label.append(label)\n",
        "\n",
        "    return data_emb, data_label\n"
      ],
      "metadata": {
        "id": "axMOkZZGVGR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "882mEDzFaYWG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7a5678e-914f-4e72-b7ba-32325db87d3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [00:31<00:00,  6.38it/s]\n",
            "100%|██████████| 100/100 [00:16<00:00,  6.22it/s]\n"
          ]
        }
      ],
      "source": [
        "# construct train, val, test dataset\n",
        "train_dataset = process_dataset(df_train, embedding=word_embedding)\n",
        "test_dataset = process_dataset(df_test, embedding=word_embedding)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sY6OL6rya_xd"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "all_pipelines = {\n",
        "    'svm': [('svm', SVM(\n",
        "                                    # probability = True,\n",
        "                                    verbose=0,\n",
        "                                    max_iter=10000,\n",
        "                                    random_state=42,\n",
        "                                    )\n",
        "                        ),\n",
        "            ],\n",
        "    'lr': [('lr', LR(\n",
        "                                    verbose=0,\n",
        "                                    max_iter=10000,\n",
        "                                    random_state=42,\n",
        "                                    )\n",
        "                        ),],\n",
        "    'rf': [('rf', RF(\n",
        "                                    verbose=0,\n",
        "                                    random_state=42,\n",
        "                                    )\n",
        "                        ),],\n",
        "    'xgb': [('xgb', XGB(\n",
        "                                    use_label_encoder=False,\n",
        "                                    verbosity=0,\n",
        "                                    random_state=42,\n",
        "                                    )\n",
        "                        ),],\n",
        "    'lgb': [('lgb', lgb.LGBMClassifier(\n",
        "                                    objective = 'binary',\n",
        "                                    verbose=0,\n",
        "                                    random_state=42,\n",
        "                                    deterministic=True,\n",
        "                                    early_stopping_rounds=200,\n",
        "                                    )\n",
        "                        ),],\n",
        "}\n",
        "\n",
        "all_parameters = {\n",
        "    'svm': {\n",
        "            'svm__C': [.1, .2, .5, 1, 2, 5, 10, 20, 50, 100],\n",
        "            'svm__kernel': ['linear', 'rbf'], # ['rbf', 'linear', 'poly', 'sigmoid']\n",
        "            # 'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "        },\n",
        "    'lr': {\n",
        "            'lr__C': [.1, .2, .5, 1, 2, 5, 10, 20, 50, 100],\n",
        "            'lr__penalty': ['l1', 'l2'],\n",
        "            'lr__solver': ['liblinear', 'saga'], # ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
        "        },\n",
        "    'rf': {\n",
        "            'rf__n_estimators': [20, 50, 100, 200, 500], # try -> [50, 100, 200, 500]\n",
        "            'rf__max_depth': [None, 5, 10, 50, 100],\n",
        "            'rf__min_samples_leaf':  [1, 2, 4],\n",
        "            'rf__max_features': ['auto'], # ['auto', 'sqrt', 'log2', None]\n",
        "            'rf__bootstrap': [True], # [True, False]\n",
        "            'rf__oob_score': [True], # [True, False]\n",
        "        },\n",
        "    'xgb': {\n",
        "            'xgb__n_estimators': [100, 200, 300],\n",
        "            'xgb__max_depth': [3, 5, 7],\n",
        "            'xgb__eta': [0.05, 0.1, 0.3],\n",
        "            \"xgb__subsample\": [0.6, 0.8, 1.0],\n",
        "            \"xgb__colsample_bytree\": [0.6, 0.8, 1.0],\n",
        "        },\n",
        "    'lgb': {\n",
        "            'lgb__boosting_type' : ['dart', 'gbdt'], # dart is good\n",
        "            'lgb__n_estimators': [50, 100, 200],\n",
        "            'lgb__learning_rate': [0.05, 0.1, 0.3],\n",
        "\n",
        "            'lgb__subsample': [0.6, 0.8, 1.0],\n",
        "\n",
        "            'lgb__colsample_bytree' : [0.6, 0.8, 1.0],\n",
        "            # 'lgb__l1_regularization' : [1,1.2],\n",
        "            # 'lgb__l2_regularization' : [1,1.2,1.4],\n",
        "            # 'lgb__num_leaves': [6, 8, 10,12, 16], # large num_leaves helps improve accuracy but might lead to over-fitting\n",
        "        },\n",
        "}\n"
      ],
      "metadata": {
        "id": "yoF4LW0kL9pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model in ('svm', 'lr', 'rf', 'lgb', 'xgb', ):\n",
        "    print(model, '>'*(32-len(model)))\n",
        "    pipeline = Pipeline(all_pipelines[model])\n",
        "    parameters = all_parameters[model]\n",
        "    \n",
        "    grid_search = GridSearchCV(\n",
        "                            estimator = pipeline, \n",
        "                            param_grid = parameters, \n",
        "                            cv = StratifiedKFold(10, shuffle=True, random_state=42),\n",
        "                            scoring = 'accuracy',\n",
        "                            n_jobs = -1,\n",
        "                            return_train_score = True,\n",
        "                            verbose = 4,\n",
        "                        )\n",
        "\n",
        "\n",
        "    # x = grid_search.fit(train_dataset[0], train_dataset[1])\n",
        "    # z = grid_search.predict(test_dataset[0])\n",
        "\n",
        "    x = grid_search.fit(np.stack(train_dataset[0]), train_dataset[1])\n",
        "    z = grid_search.predict(np.stack(test_dataset[0]))\n",
        "\n",
        "    print(classification_report(test_dataset[1], z, digits=4))\n",
        "\n",
        "    print(grid_search.best_params_)\n",
        "\n",
        "    print('<'*32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RFqwJthL9ke",
        "outputId": "7c3e3a25-c912-4538-9e5b-5b17251ad771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "svm >>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7347    0.7200    0.7273        50\n",
            "           1     0.7255    0.7400    0.7327        50\n",
            "\n",
            "    accuracy                         0.7300       100\n",
            "   macro avg     0.7301    0.7300    0.7300       100\n",
            "weighted avg     0.7301    0.7300    0.7300       100\n",
            "\n",
            "{'svm__C': 0.5, 'svm__kernel': 'linear'}\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "lr >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7826    0.7200    0.7500        50\n",
            "           1     0.7407    0.8000    0.7692        50\n",
            "\n",
            "    accuracy                         0.7600       100\n",
            "   macro avg     0.7617    0.7600    0.7596       100\n",
            "weighted avg     0.7617    0.7600    0.7596       100\n",
            "\n",
            "{'lr__C': 1, 'lr__penalty': 'l1', 'lr__solver': 'liblinear'}\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "rf >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Fitting 10 folds for each of 75 candidates, totalling 750 fits\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6957    0.6400    0.6667        50\n",
            "           1     0.6667    0.7200    0.6923        50\n",
            "\n",
            "    accuracy                         0.6800       100\n",
            "   macro avg     0.6812    0.6800    0.6795       100\n",
            "weighted avg     0.6812    0.6800    0.6795       100\n",
            "\n",
            "{'rf__bootstrap': True, 'rf__max_depth': None, 'rf__max_features': 'auto', 'rf__min_samples_leaf': 4, 'rf__n_estimators': 50, 'rf__oob_score': True}\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "lgb >>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Fitting 10 folds for each of 162 candidates, totalling 1620 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "810 fits failed out of a total of 1620.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "810 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 394, in fit\n",
            "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/lightgbm/sklearn.py\", line 744, in fit\n",
            "    callbacks=callbacks)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/lightgbm/sklearn.py\", line 544, in fit\n",
            "    callbacks=callbacks)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py\", line 233, in train\n",
            "    evaluation_result_list=evaluation_result_list))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/lightgbm/callback.py\", line 211, in _callback\n",
            "    _init(env)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/lightgbm/callback.py\", line 192, in _init\n",
            "    raise ValueError('For early stopping, '\n",
            "ValueError: For early stopping, at least one dataset and eval metric is required for evaluation\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [0.735 0.735 0.735 0.745 0.745 0.745 0.76  0.76  0.76  0.75  0.75  0.75\n",
            " 0.745 0.745 0.745 0.77  0.77  0.77  0.775 0.775 0.775 0.74  0.74  0.74\n",
            " 0.75  0.75  0.75  0.75  0.75  0.75  0.75  0.75  0.75  0.755 0.755 0.755\n",
            " 0.765 0.765 0.765 0.745 0.745 0.745 0.785 0.785 0.785 0.735 0.735 0.735\n",
            " 0.745 0.745 0.745 0.735 0.735 0.735 0.755 0.755 0.755 0.755 0.755 0.755\n",
            " 0.76  0.76  0.76  0.77  0.77  0.77  0.755 0.755 0.755 0.74  0.74  0.74\n",
            " 0.725 0.725 0.725 0.735 0.735 0.735 0.705 0.705 0.705   nan   nan   nan\n",
            "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
            "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
            "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
            "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
            "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
            "   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan   nan\n",
            "   nan   nan   nan   nan   nan   nan]\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the train scores are non-finite: [0.91444444 0.91444444 0.91444444 0.94555556 0.94555556 0.94555556\n",
            " 0.97388889 0.97388889 0.97388889 0.965      0.965      0.965\n",
            " 0.98555556 0.98555556 0.98555556 0.99611111 0.99611111 0.99611111\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.         0.90222222 0.90222222 0.90222222\n",
            " 0.93777778 0.93777778 0.93777778 0.96888889 0.96888889 0.96888889\n",
            " 0.96388889 0.96388889 0.96388889 0.98888889 0.98888889 0.98888889\n",
            " 0.99777778 0.99777778 0.99777778 1.         1.         1.\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 0.90333333 0.90333333 0.90333333 0.93555556 0.93555556 0.93555556\n",
            " 0.96888889 0.96888889 0.96888889 0.96388889 0.96388889 0.96388889\n",
            " 0.99       0.99       0.99       0.99888889 0.99888889 0.99888889\n",
            " 1.         1.         1.         1.         1.         1.\n",
            " 1.         1.         1.                nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan]\n",
            "  category=UserWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/engine.py:123: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
            "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
            "/usr/local/lib/python3.7/dist-packages/lightgbm/callback.py:189: UserWarning: Early stopping is not available in dart mode\n",
            "  warnings.warn('Early stopping is not available in dart mode')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7021    0.6600    0.6804        50\n",
            "           1     0.6792    0.7200    0.6990        50\n",
            "\n",
            "    accuracy                         0.6900       100\n",
            "   macro avg     0.6907    0.6900    0.6897       100\n",
            "weighted avg     0.6907    0.6900    0.6897       100\n",
            "\n",
            "{'lgb__boosting_type': 'dart', 'lgb__colsample_bytree': 0.8, 'lgb__learning_rate': 0.1, 'lgb__n_estimators': 200, 'lgb__subsample': 0.6}\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "xgb >>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Fitting 10 folds for each of 243 candidates, totalling 2430 fits\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6596    0.6200    0.6392        50\n",
            "           1     0.6415    0.6800    0.6602        50\n",
            "\n",
            "    accuracy                         0.6500       100\n",
            "   macro avg     0.6505    0.6500    0.6497       100\n",
            "weighted avg     0.6505    0.6500    0.6497       100\n",
            "\n",
            "{'xgb__colsample_bytree': 0.6, 'xgb__eta': 0.05, 'xgb__max_depth': 5, 'xgb__n_estimators': 300, 'xgb__subsample': 1.0}\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# grid_search.best_params_"
      ],
      "metadata": {
        "id": "I2GwpWrmN3a0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jy1ZQ-b85yKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier"
      ],
      "metadata": {
        "id": "gp-0tfSQ5yFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline([\n",
        "                     ('nn', MLPClassifier(\n",
        "                                max_iter=10000, \n",
        "                                solver=\"lbfgs\",\n",
        "                                random_state=42, \n",
        "                                )\n",
        "                     )\n",
        "])\n",
        "\n",
        "parameters = {\n",
        "    'nn__hidden_layer_sizes': [ (16,), (8,), (4,), (16, 8), (8, 4), (16, 8, 4),  ],\n",
        "    'nn__activation': ['tanh', 'relu', 'identity'],\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "                            estimator = pipeline, \n",
        "                            param_grid = parameters, \n",
        "                            cv = StratifiedKFold(10, shuffle=True, random_state=42),\n",
        "                            scoring = 'accuracy',\n",
        "                            n_jobs = -1,\n",
        "                            return_train_score = True,\n",
        "                            verbose = 4,\n",
        "                        )\n",
        "\n",
        "\n",
        "x = grid_search.fit(train_dataset[0], train_dataset[1])\n",
        "z = grid_search.predict(test_dataset[0])\n",
        "print(classification_report(test_dataset[1], z, digits=4))\n",
        "\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "9GYwN-oBaHTx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7f91f55-c3f0-4390-c8c7-16b848a1e036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 10 folds for each of 18 candidates, totalling 180 fits\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6042    0.5800    0.5918        50\n",
            "           1     0.5962    0.6200    0.6078        50\n",
            "\n",
            "    accuracy                         0.6000       100\n",
            "   macro avg     0.6002    0.6000    0.5998       100\n",
            "weighted avg     0.6002    0.6000    0.5998       100\n",
            "\n",
            "{'nn__activation': 'tanh', 'nn__hidden_layer_sizes': (16, 8, 4)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "buSmj0EQ8KwA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "[final]btp_our_model_01",
      "provenance": [],
      "mount_file_id": "1XNB1z2ZGReTC3P93AObndZmXjK_Lc4Nz",
      "authorship_tag": "ABX9TyPa80M54ewTAt+oMEahK3+M",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}