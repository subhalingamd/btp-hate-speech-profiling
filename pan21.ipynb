{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.pan21 import read_dataset\n",
    "\n",
    "path = './data/pan21/train/en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_dataset(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()    # labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 200)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[0]), len(data[0][0])    # (num_users_in_a_label, num_tweets_per_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eda.get_stats import *\n",
    "from eda.get_per_user_stats import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#HASHTAG#: {0: 3757, 1: 3392}\n",
      "#URL#: {0: 8571, 1: 6768}\n",
      "#USER#: {0: 9723, 1: 11571}\n",
      "##RT##: {0: 7633, 1: 6090}\n"
     ]
    }
   ],
   "source": [
    "hashtags = count_hashtags(data)\n",
    "urls = count_urls(data)\n",
    "users = count_users(data)\n",
    "rt = count_rt(data)\n",
    "\n",
    "\n",
    "# print out\n",
    "print('#HASHTAG#:', hashtags)\n",
    "print('#URL#:', urls)\n",
    "print('#USER#:', users)\n",
    "print('##RT##:', rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of upper-case characters: {0: 71025, 1: 75867}\n",
      "min min characters: {0: 4, 1: 5}\n",
      "avg min characters: {0: 10.28, 1: 10.55}\n",
      "max max characters: {0: 143, 1: 148}\n",
      "avg max characters: {0: 125.08, 1: 128.2}\n",
      "number of characters: {0: 1109779, 1: 1134313}\n"
     ]
    }
   ],
   "source": [
    "uppercase_chars = count_uppercase_chars(data)\n",
    "min_chars = count_min_chars(data)\n",
    "avg_min_chars = count_avg_min_chars(data)\n",
    "max_chars = count_max_chars(data)\n",
    "avg_max_chars = count_avg_max_chars(data)\n",
    "chars = count_chars(data)\n",
    "\n",
    "# print out\n",
    "print('number of upper-case characters:', uppercase_chars)\n",
    "print('min min characters:', min_chars)\n",
    "print('avg min characters:', avg_min_chars)\n",
    "print('max max characters:', max_chars)\n",
    "print('avg max characters:', avg_max_chars)\n",
    "print('number of characters:', chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of upper-case words: {0: 43584, 1: 44012}\n",
      "min min words: {0: 1, 1: 1}\n",
      "avg min words: {0: 2.53, 1: 2.68}\n",
      "max max words: {0: 31, 1: 32}\n",
      "avg max words: {0: 24.85, 1: 25.58}\n",
      "number of words: {0: 207317, 1: 213744}\n"
     ]
    }
   ],
   "source": [
    "uppercase_words = count_uppercase_words(data)\n",
    "min_words = count_min_words(data)\n",
    "avg_min_words = count_avg_min_words(data)\n",
    "max_words = count_max_words(data)\n",
    "avg_max_words = count_avg_max_words(data)\n",
    "words = count_words(data)\n",
    "\n",
    "# print out\n",
    "print('number of upper-case words:', uppercase_words)\n",
    "print('min min words:', min_words)\n",
    "print('avg min words:', avg_min_words)\n",
    "print('max max words:', max_words)\n",
    "print('avg max words:', avg_max_words)\n",
    "print('number of words:', words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min min words (alt): {0: 1, 1: 2}\n",
      "avg min words (alt): {0: 3.29, 1: 3.5}\n",
      "max max words (alt): {0: 32, 1: 32}\n",
      "avg max words (alt): {0: 25.37, 1: 26.07}\n",
      "number of words (alt): {0: 228644, 1: 234867}\n"
     ]
    }
   ],
   "source": [
    "# include #HASHTAG#, #URL#, #USER#\n",
    "# not same as count_uppercase_words() + count(#HASHTAG#, #URL#, #USER#)\n",
    "# not all #...# are followed by space. e.g., #USER#_Daily\n",
    "\n",
    "min_words_alt = count_min_words_alt(data)\n",
    "avg_min_words_alt = count_avg_min_words_alt(data)\n",
    "max_words_alt = count_max_words_alt(data)\n",
    "avg_max_words_alt = count_avg_max_words_alt(data)\n",
    "words_alt = count_words_alt(data)\n",
    "\n",
    "# print out\n",
    "print('min min words (alt):', min_words_alt)\n",
    "print('avg min words (alt):', avg_min_words_alt)\n",
    "print('max max words (alt):', max_words_alt)\n",
    "print('avg max words (alt):', avg_max_words_alt)\n",
    "print('number of words (alt):', words_alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = count_stopwords(data)\n",
    "\n",
    "# print out\n",
    "print('number of stop-words:', stop_words)\n",
    "\n",
    "## just split: number of stop-words: {0: 95998, 1: 101886}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emojis = count_emojis(data)\n",
    "\n",
    "# print out\n",
    "print('number of emojis:', emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = get_sentiments(data)\n",
    "\n",
    "# print out\n",
    "print('sentiment:', sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner1 = get_named_entities(data, corpora='en_core_web_sm')\n",
    "ner2 = get_named_entities(data, corpora='xx_ent_wiki_sm')\n",
    "\n",
    "# print out\n",
    "print('ner1', ner1)\n",
    "print()\n",
    "print('ner2', ner2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Per user stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_user_hashtags = count_per_user_hashtags(data)\n",
    "per_user_urls = count_per_user_urls(data)\n",
    "per_user_users = count_per_user_users(data)\n",
    "per_user_rt = count_per_user_rt(data)\n",
    "\n",
    "assert {label: sum(v) for label, v in per_user_hashtags.items()} == hashtags\n",
    "assert {label: sum(v) for label, v in per_user_urls.items()} == urls\n",
    "assert {label: sum(v) for label, v in per_user_users.items()} == users\n",
    "assert {label: sum(v) for label, v in per_user_rt.items()} == rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_user_uppercase_chars = count_per_user_uppercase_chars(data)\n",
    "per_user_min_chars = count_per_user_min_chars(data)\n",
    "per_user_max_chars = count_per_user_max_chars(data)\n",
    "per_user_chars = count_per_user_chars(data)\n",
    "\n",
    "assert {label: sum(v) for label, v in per_user_uppercase_chars.items()} == uppercase_chars\n",
    "assert {label: min(v) for label, v in per_user_min_chars.items()} == min_chars\n",
    "assert {label: sum(v)/len(v) for label, v in per_user_min_chars.items()} == avg_min_chars\n",
    "assert {label: max(v) for label, v in per_user_max_chars.items()} == max_chars\n",
    "assert {label: sum(v)/len(v) for label, v in per_user_max_chars.items()} == avg_max_chars\n",
    "assert {label: sum(v) for label, v in per_user_chars.items()} == chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_user_uppercase_words = count_per_user_uppercase_words(data)\n",
    "per_user_min_words = count_per_user_min_words(data)\n",
    "per_user_max_words = count_per_user_max_words(data)\n",
    "per_user_words = count_per_user_words(data)\n",
    "\n",
    "assert {label: sum(v) for label, v in per_user_uppercase_words.items()} == uppercase_words\n",
    "assert {label: min(v) for label, v in per_user_min_words.items()} == min_words\n",
    "assert {label: sum(v)/len(v) for label, v in per_user_min_words.items()} == avg_min_words\n",
    "assert {label: max(v) for label, v in per_user_max_words.items()} == max_words\n",
    "assert {label: sum(v)/len(v) for label, v in per_user_max_words.items()} == avg_max_words\n",
    "assert {label: sum(v) for label, v in per_user_words.items()} == words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_user_min_words_alt = count_per_user_min_words_alt(data)\n",
    "per_user_max_words_alt = count_per_user_max_words_alt(data)\n",
    "per_user_words_alt = count_per_user_words_alt(data)\n",
    "\n",
    "assert {label: min(v) for label, v in per_user_min_words_alt.items()} == min_words_alt\n",
    "assert {label: sum(v)/len(v) for label, v in per_user_min_words_alt.items()} == avg_min_words_alt\n",
    "assert {label: max(v) for label, v in per_user_max_words_alt.items()} == max_words_alt\n",
    "assert {label: sum(v)/len(v) for label, v in per_user_max_words_alt.items()} == avg_max_words_alt\n",
    "assert {label: sum(v) for label, v in per_user_words_alt.items()} == words_alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_user_stopwords = count_per_user_stopwords(data)\n",
    "\n",
    "assert {label: sum(v) for label, v in per_user_stopwords.items()} == stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_user_emojis = count_per_user_emojis(data)\n",
    "\n",
    "assert {label: sum(v) for label, v in per_user_emojis.items()} == emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_user_sentiments = get_per_user_sentiments(data)\n",
    "\n",
    "for sentiment_type, sentiment_value in per_user_sentiments.items():\n",
    "    assert {label: sum(v) for label, v in sentiment_value.items()} == sentiments[sentiment_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_user_ner1 = get_per_user_named_entities(data, corpora='en_core_web_sm')\n",
    "per_user_ner2 = get_per_user_named_entities(data, corpora='xx_ent_wiki_sm')\n",
    "\n",
    "for ner_type, ner_value in per_user_ner1.items():\n",
    "    assert {label: sum(v) for label, v in ner_value.items()} == ner1[ner_type]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df of features for each label\n",
    "df_hashtags = pd.DataFrame(per_user_hashtags)\n",
    "df_urls = pd.DataFrame(per_user_urls)\n",
    "df_users = pd.DataFrame(per_user_users)\n",
    "df_rt = pd.DataFrame(per_user_rt)\n",
    "df_uppercase_chars = pd.DataFrame(per_user_uppercase_chars)\n",
    "df_min_chars = pd.DataFrame(per_user_min_chars)\n",
    "df_max_chars = pd.DataFrame(per_user_max_chars)\n",
    "df_chars = pd.DataFrame(per_user_chars)\n",
    "df_uppercase_words = pd.DataFrame(per_user_uppercase_words)\n",
    "df_min_words = pd.DataFrame(per_user_min_words)\n",
    "df_max_words = pd.DataFrame(per_user_max_words)\n",
    "df_words = pd.DataFrame(per_user_words)\n",
    "df_min_words_alt = pd.DataFrame(per_user_min_words_alt)\n",
    "df_max_words_alt = pd.DataFrame(per_user_max_words_alt)\n",
    "df_words_alt = pd.DataFrame(per_user_words_alt)\n",
    "df_stop_words = pd.DataFrame(per_user_stopwords)\n",
    "df_emojis = pd.DataFrame(per_user_emojis)\n",
    "df_sentiments_positive = pd.DataFrame(per_user_sentiments['positive'])\n",
    "df_sentiments_negative = pd.DataFrame(per_user_sentiments['negative'])\n",
    "df_sentiments_neutral = pd.DataFrame(per_user_sentiments['neutral'])\n",
    "df_ner1_PERSON = pd.DataFrame(per_user_ner1['PERSON'])\n",
    "df_ner1_ORG = pd.DataFrame(per_user_ner1['ORG'])\n",
    "df_ner1_LOC = pd.DataFrame(per_user_ner1['LOC']) + pd.DataFrame(per_user_ner1['GPE'])\n",
    "df_ner1_MISC = pd.DataFrame(per_user_ner1['MISC'])\n",
    "df_ner2_PER = pd.DataFrame(per_user_ner2['PER'])\n",
    "df_ner2_ORG = pd.DataFrame(per_user_ner2['ORG'])\n",
    "df_ner2_LOC = pd.DataFrame(per_user_ner2['LOC'])\n",
    "df_ner2_MISC = pd.DataFrame(per_user_ner2['MISC'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot frequency of features in each label in same plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(df_hashtags[1].value_counts().sort_index() - df_hashtags[0].value_counts().sort_index()).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv without index\n",
    "df_hashtags.to_csv('eda/per_user_stats/hashtags.csv', index=False)\n",
    "df_urls.to_csv('eda/per_user_stats/urls.csv', index=False)\n",
    "df_users.to_csv('eda/per_user_stats/users.csv', index=False)\n",
    "df_rt.to_csv('eda/per_user_stats/rt.csv', index=False)\n",
    "df_uppercase_chars.to_csv('eda/per_user_stats/uppercase_chars.csv', index=False)\n",
    "df_min_chars.to_csv('eda/per_user_stats/min_chars.csv', index=False)\n",
    "df_max_chars.to_csv('eda/per_user_stats/max_chars.csv', index=False)\n",
    "df_chars.to_csv('eda/per_user_stats/chars.csv', index=False)\n",
    "df_uppercase_words.to_csv('eda/per_user_stats/uppercase_words.csv', index=False)\n",
    "df_min_words.to_csv('eda/per_user_stats/min_words.csv', index=False)\n",
    "df_max_words.to_csv('eda/per_user_stats/max_words.csv', index=False)\n",
    "df_words.to_csv('eda/per_user_stats/words.csv', index=False)\n",
    "df_min_words_alt.to_csv('eda/per_user_stats/min_words_alt.csv', index=False)\n",
    "df_max_words_alt.to_csv('eda/per_user_stats/max_words_alt.csv', index=False)\n",
    "df_words_alt.to_csv('eda/per_user_stats/words_alt.csv', index=False)\n",
    "df_stop_words.to_csv('eda/per_user_stats/stop_words.csv', index=False)\n",
    "df_emojis.to_csv('eda/per_user_stats/emojis.csv', index=False)\n",
    "df_sentiments_positive.to_csv('eda/per_user_stats/sentiments_positive.csv', index=False)\n",
    "df_sentiments_negative.to_csv('eda/per_user_stats/sentiments_negative.csv', index=False)\n",
    "df_sentiments_neutral.to_csv('eda/per_user_stats/sentiments_neutral.csv', index=False)\n",
    "df_ner1_PERSON.to_csv('eda/per_user_stats/ner1_PERSON.csv', index=False)\n",
    "df_ner1_ORG.to_csv('eda/per_user_stats/ner1_ORG.csv', index=False)\n",
    "df_ner1_LOC.to_csv('eda/per_user_stats/ner1_LOCATION.csv', index=False)\n",
    "df_ner1_MISC.to_csv('eda/per_user_stats/ner1_MISC.csv', index=False)\n",
    "df_ner2_PER.to_csv('eda/per_user_stats/ner2_PER.csv', index=False)\n",
    "df_ner2_ORG.to_csv('eda/per_user_stats/ner2_ORG.csv', index=False)\n",
    "df_ner2_LOC.to_csv('eda/per_user_stats/ner2_LOC.csv', index=False)\n",
    "df_ner2_MISC.to_csv('eda/per_user_stats/ner2_MISC.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save describe to csv with index\n",
    "df_hashtags.describe().to_csv('eda/per_user_stats/describe/hashtags.csv', index=True)\n",
    "df_urls.describe().to_csv('eda/per_user_stats/describe/urls.csv', index=True)\n",
    "df_users.describe().to_csv('eda/per_user_stats/describe/users.csv', index=True)\n",
    "df_rt.describe().to_csv('eda/per_user_stats/describe/rt.csv', index=True)\n",
    "df_uppercase_chars.describe().to_csv('eda/per_user_stats/describe/uppercase_chars.csv', index=True)\n",
    "df_min_chars.describe().to_csv('eda/per_user_stats/describe/min_chars.csv', index=True)\n",
    "df_max_chars.describe().to_csv('eda/per_user_stats/describe/max_chars.csv', index=True)\n",
    "df_chars.describe().to_csv('eda/per_user_stats/describe/chars.csv', index=True)\n",
    "df_uppercase_words.describe().to_csv('eda/per_user_stats/describe/uppercase_words.csv', index=True)\n",
    "df_min_words.describe().to_csv('eda/per_user_stats/describe/min_words.csv', index=True)\n",
    "df_max_words.describe().to_csv('eda/per_user_stats/describe/max_words.csv', index=True)\n",
    "df_words.describe().to_csv('eda/per_user_stats/describe/words.csv', index=True)\n",
    "df_min_words_alt.describe().to_csv('eda/per_user_stats/describe/min_words_alt.csv', index=True)\n",
    "df_max_words_alt.describe().to_csv('eda/per_user_stats/describe/max_words_alt.csv', index=True)\n",
    "df_words_alt.describe().to_csv('eda/per_user_stats/describe/words_alt.csv', index=True)\n",
    "df_stop_words.describe().to_csv('eda/per_user_stats/describe/stop_words.csv', index=True)\n",
    "df_emojis.describe().to_csv('eda/per_user_stats/describe/emojis.csv', index=True)\n",
    "df_sentiments_positive.describe().to_csv('eda/per_user_stats/describe/sentiments_positive.csv', index=True)\n",
    "df_sentiments_negative.describe().to_csv('eda/per_user_stats/describe/sentiments_negative.csv', index=True)\n",
    "df_sentiments_neutral.describe().to_csv('eda/per_user_stats/describe/sentiments_neutral.csv', index=True)\n",
    "df_ner1_PERSON.describe().to_csv('eda/per_user_stats/describe/ner1_PERSON.csv', index=True)\n",
    "df_ner1_ORG.describe().to_csv('eda/per_user_stats/describe/ner1_ORG.csv', index=True)\n",
    "df_ner1_LOC.describe().to_csv('eda/per_user_stats/describe/ner1_LOCATION.csv', index=True)\n",
    "df_ner1_MISC.describe().to_csv('eda/per_user_stats/describe/ner1_MISC.csv', index=True)\n",
    "df_ner2_PER.describe().to_csv('eda/per_user_stats/describe/ner2_PER.csv', index=True)\n",
    "df_ner2_ORG.describe().to_csv('eda/per_user_stats/describe/ner2_ORG.csv', index=True)\n",
    "df_ner2_LOC.describe().to_csv('eda/per_user_stats/describe/ner2_LOC.csv', index=True)\n",
    "df_ner2_MISC.describe().to_csv('eda/per_user_stats/describe/ner2_MISC.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df for each label\n",
    "for label in data.keys():\n",
    "    pd.DataFrame(data={\n",
    "        \"hashtags\": df_hashtags[label],\n",
    "        \"urls\": df_urls[label],\n",
    "        \"users\": df_users[label],\n",
    "        \"rt\": df_rt[label],\n",
    "        \"uppercase_chars\": df_uppercase_chars[label],\n",
    "        \"min_chars\": df_min_chars[label],\n",
    "        \"max_chars\": df_max_chars[label],\n",
    "        \"chars\": df_chars[label],\n",
    "        \"uppercase_words\": df_uppercase_words[label],\n",
    "        \"min_words\": df_min_words[label],\n",
    "        \"max_words\": df_max_words[label],\n",
    "        \"words\": df_words[label],\n",
    "        \"min_words_alt\": df_min_words_alt[label],\n",
    "        \"max_words_alt\": df_max_words_alt[label],\n",
    "        \"words_alt\": df_words_alt[label],\n",
    "        \"stop_words\": df_stop_words[label],\n",
    "        \"emojis\": df_emojis[label],\n",
    "        \"sentiments_positive\": df_sentiments_positive[label],\n",
    "        \"sentiments_negative\": df_sentiments_negative[label],\n",
    "        \"sentiments_neutral\": df_sentiments_neutral[label],\n",
    "        \"ner1_PERSON\": df_ner1_PERSON[label],\n",
    "        \"ner1_ORG\": df_ner1_ORG[label],\n",
    "        \"ner1_LOC\": df_ner1_LOC[label],\n",
    "        \"ner1_MISC\": df_ner1_MISC[label],\n",
    "        \"ner2_PER\": df_ner2_PER[label],\n",
    "        \"ner2_ORG\": df_ner2_ORG[label],\n",
    "        \"ner2_LOC\": df_ner2_LOC[label],\n",
    "        \"ner2_MISC\": df_ner2_MISC[label]\n",
    "    }).to_csv(f'eda/per_user_stats/combined/label_{label}.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.971350</td>\n",
       "      <td>0.992200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.103418</td>\n",
       "      <td>0.028216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.155000</td>\n",
       "      <td>0.775000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.995000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0           1\n",
       "count  100.000000  100.000000\n",
       "mean     0.971350    0.992200\n",
       "std      0.103418    0.028216\n",
       "min      0.155000    0.775000\n",
       "25%      0.990000    0.995000\n",
       "50%      1.000000    1.000000\n",
       "75%      1.000000    1.000000\n",
       "max      1.000000    1.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique tweets ratio\n",
    "\n",
    "unique_tweets_ratio = get_unique_tweets_ratio(data)\n",
    "pd.DataFrame(unique_tweets_ratio).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.975     1\n",
       "0.885     1\n",
       "0.890     1\n",
       "0.840     1\n",
       "0.770     1\n",
       "0.875     1\n",
       "0.910     1\n",
       "0.970     1\n",
       "0.480     1\n",
       "0.930     1\n",
       "0.940     1\n",
       "0.945     1\n",
       "0.155     1\n",
       "0.925     1\n",
       "0.980     2\n",
       "0.985     3\n",
       "0.955     4\n",
       "0.990     5\n",
       "0.995     8\n",
       "1.000    64\n",
       "Name: 0, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.970     1\n",
       "0.775     1\n",
       "0.975     1\n",
       "0.935     1\n",
       "0.850     1\n",
       "0.955     2\n",
       "0.985     5\n",
       "0.990     5\n",
       "0.995    14\n",
       "1.000    69\n",
       "Name: 1, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "unique_tweets_ratio = get_unique_tweets_ratio(data)\n",
    "# pd.DataFrame({k: sorted(v) for k,v in unique_tweets_ratio.items()}).value_counts()#.to_csv('eda/unique_ratio__sorted.csv')\n",
    "\n",
    "d_ = pd.DataFrame(unique_tweets_ratio)\n",
    "for i in range(2):\n",
    "    display(d_[i].value_counts().sort_values(inplace=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n",
      "['I’m just being me under construction #HASHTAG# on the way #URL#', 'one person followed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person followed me and one person unfollowed me // automatically checked by #URL#', 'My birthday coming up and shit it’s lit', 'one person unfollowed me // automatically checked by #URL#', 'I don’t post on here so fuck it #URL#', 'one person followed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person unfollowed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person unfollowed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person unfollowed me // automatically checked by #URL#', 'one person unfollowed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'Niggas ain’t trying portray a image we just trying become our best selves #URL#', '2 people followed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person unfollowed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'This shit be all facts 💯💯💯 #URL#', '##RT## i need a girl friend all to myself.', '##RT## Snippet from the song called stick play dropping on my tape called I’m just being me #URL#', 'one person followed me // automatically checked by #URL#', 'If you going do it do it 100 percent', '##RT## Them tables definitely turn . U gotta b as humble as possible', 'This shit going be a hit 💯 #URL#', 'one person unfollowed me // automatically checked by #URL#', 'I’m really finding my artistry', '#USER# #USER# You throwing celebrities bull shit outta proportion 😂💯', 'Bad Idea 😂😂 #URL#', 'I been different', 'Yeah I really been on my shit 💯💯💯 Tune in 🖤🙅🏾\\u200d♂️🧢 #URL#', 'Snippet from the song called stick play dropping on my tape called I’m just being me #URL#', '2 people unfollowed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', '##RT## 2020 was a brutal year 💔', 'Been off my grown man shit Lately getting money and staying out da way 💯💯💯', 'Net working is really the key', 'Ain’t been on here in for ever 😂', 'Show me some love real quick Lls', 'one person followed me // automatically checked by #URL#', 'one person unfollowed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person unfollowed me // automatically checked by #URL#', '2 people followed me // automatically checked by #URL#', 'one person followed me and one person unfollowed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person unfollowed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person unfollowed me // automatically checked by #URL#', 'one person followed me and one person unfollowed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person unfollowed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person unfollowed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'Trying see a 100 bands off this street shit and we going get it 💯❌🧢', '4 people followed me // automatically checked by #URL#', 'one person unfollowed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', '#USER# Damn they give America girls a chance to have an exotic pussy 😂😂😂😁', '2 people followed me // automatically checked by #URL#', '#USER# Worst mistake of your life 🥶😭', 'one person followed me // automatically checked by #URL#', 'one person unfollowed me // automatically checked by #URL#', 'I used to wanna be famous as long as I got this paper I can remain nameless 💯 #URL#', 'one person followed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person unfollowed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', '##RT## League source: Former #HASHTAG# pick Hasheem Thabeet is working out for the Bucks next week! (Via #USER#)', 'one person unfollowed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person unfollowed me // automatically checked by #URL#', 'Shooting thus video next 💯 #URL#', 'one person unfollowed me // automatically checked by #URL#', '2 people followed me // automatically checked by #URL#', '##RT## Im getting back on my feet fuck niggas better watch out 💯 #URL#', 'Im getting back on my feet fuck niggas better watch out 💯 #URL#', 'one person followed me // automatically checked by #URL#', 'one person followed me and one person unfollowed me // automatically checked by #URL#', '2 people followed me // automatically checked by #URL#', 'one person unfollowed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person unfollowed me // automatically checked by #URL#', '3 people followed me // automatically checked by #URL#', 'one person unfollowed me // automatically checked by #URL#', '2 people followed me // automatically checked by #URL#', 'one person followed me and 2 people unfollowed me // automatically checked by #URL#', 'one person unfollowed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person unfollowed me // automatically checked by #URL#', 'one person unfollowed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', '##RT## Yeah this shit dropping soon rt and like if you enjoy #URL#', 'one person followed me // automatically checked by #URL#', 'one person unfollowed me // automatically checked by #URL#', 'one person unfollowed me // automatically checked by #URL#', 'one person followed me and one person unfollowed me // automatically checked by #URL#', '2 people followed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'Yup we all dealing with it 😤😭 #URL#', '#USER# Keep going brodie 🤙💯🔥', 'one person followed me // automatically checked by #URL#', 'This is going to be a good year 🤙💯 #URL#', 'Thank you 💕 #URL#', \"##RT## It's to easy 😎💪🏽 #URL#\", 'If you 21 come show me some love tonight dm me for more infor #URL#', 'Not just a cheating but a lost generation in whole #URL#', '##RT## Wish a young nigga a happy birthday if you dont mind 😅', 'Wish a young nigga a happy birthday if you dont mind 😅', 'Whats it about? #URL#', '#USER# What the fuck she get hit with', 'Click the link in my bio 🤞✊💯 #URL#', 'one person unfollowed me // automatically checked by #URL#', '#USER# Now look at him shit crazy aint it.... Stop telling on your self with music the feds listen', 'one person followed me and one person unfollowed me // automatically checked by #URL#', '##RT## Yeah 20 years later #HASHTAG# #URL#', 'Dont trust no nigga geeking for fame #URL#', 'Yeah 20 years later #HASHTAG# #URL#', '#USER# You i been doing this shit since ninth grade faxxx💯', 'Im performing live tonight if you in dc come out its going be litt #URL#', '#USER# birthday twin you gotta smoke with me on our gday 💯', '2 people followed me // automatically checked by #URL#', '2 people followed me and one person unfollowed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', '#USER# hey people are complaining about not being able to cash out after making $150', 'one person unfollowed me // automatically checked by #URL#', 'one person followed me // automatically checked by #URL#', 'one person unfollowed me // automatically checked by #URL#', '##RT## -. we do walk ups we don’t drive by😈🤫', '##RT## i need sex consistently.', 'one person unfollowed me // automatically checked by #URL#', 'one person unfollowed me // automatically checked by #URL#', '##RT## #USER# Sorry bro but we not rockin today', 'one person unfollowed me // automatically checked by #URL#', 'Happy birthday charity #URL#', '##RT## 100 RT nd I drop this weekend 🔥🔥💯fr doe #URL#', '2 people unfollowed me // automatically checked by #URL#', \"##RT## Fuck all u bitch ass niggas that's watching ur man starve 💯💯\", 'one person followed me // automatically checked by #URL#', '#USER# Twitter is oc im gone 😂😂😂😭', '#USER# Sign me up for the plant milk after that', 'Get them the fuck outta here 😂😂😂😂 fuck ass steelers fans', '#USER# Fuck that shit', '##RT## Idk who need see this but STOP SMOKING NASTY ASS CIGARETTES!!! #URL#', '##RT## I’m not attracted to thot bitches...', '##RT## 😂😂😂😂😂 yeah fuck that i rather face 💯 niggas jays dont look like mine #URL#', '2 people unfollowed me // automatically checked by #URL#', 'Go like and retweet 💯 #URL#', 'If you dont fuck with me cant get mad about what i do', '#USER# You looking mad gorgeous right there 😂😁', '##RT## Ask me what i been up too anit shit i been cooling 2019 coming soon that new justo dropping tune in', '##RT## If my brothers ever down bad they know i got em', 'Ask me what i been up too anit shit i been cooling 2019 coming soon that new justo dropping tune in #URL#', 'one person followed me // automatically checked by #URL#', 'When you think your finessing life 😂 #URL#', 'one person followed me // automatically checked by #URL#', 'Not cole or herbo everybody else make room for justo #URL#', '##RT## We ain’t got time for the lames, we tryna live in the moment', 'You gotta perform #URL#', '##RT## Retweet for a financially stable 2019 #URL#', 'If your name dont start with a s and end with a e dont text me i only wat her', 'Lord knows i only want you stop faking and come get your real nigga you been waiting for', 'one person followed me // automatically checked by #URL#', '##RT## Da music y’all call “crankage” fucking sucks', 'Yea this my gang righ here twitter #URL#', '##RT## Only dope rappers/singers can RT this. :)', 'To everyone i fuck with keep working shit about to change 💯💯💯', 'Go run them views up click the link in my bio', 'one person followed me and one person unfollowed me // automatically checked by #URL#', 'Some body tell simone i miss her', '#USER# Love for life bj soon as i make it on god ima make sure you come with me', '##RT## #USER# Proud of you bro. Wish I can make it', 'I got many more to come this just the begging #URL#', 'Out now link in my bio #URL#', 'Mood for tonight come support the kid #URL#']\n"
     ]
    }
   ],
   "source": [
    "def get_unique_tweets_ratio_(data):\n",
    "    # given data: {label: [[tweets]]\n",
    "    # get ratio of unique tweets for each label\n",
    "    # output out: {label: [count]}\n",
    "    out = {}\n",
    "    for label, users in data.items():\n",
    "        out[label] = []\n",
    "        for user in users:\n",
    "            out[label] += [len(set(user))/len(user)]\n",
    "            if out[label][-1] == 0.480:\n",
    "                print(len(set(user)))\n",
    "                # print((set(user)))\n",
    "                print(((user)))\n",
    "                return\n",
    "    return out\n",
    "                             \n",
    "get_unique_tweets_ratio_(data)                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOT ALL #...# is followed by space\n",
    "'''e.g.\n",
    "\t\t<document><![CDATA[There was a 5sos meet up in Belgium! #HASHTAG# #HASHTAG# #USER# 💗 💗 #URL# (#USER#_Daily ) -E]]></document>\n",
    "'''\n",
    "\n",
    "def count_max_words_alt_(data):\n",
    "    # given data: {label: [[tweets]]\n",
    "    # remove ##RT##\n",
    "    # get total number of words for each label\n",
    "    # output out: {label: count}\n",
    "    out = {}\n",
    "    for label, users in data.items():\n",
    "        out[label] = 0\n",
    "        for user in users:\n",
    "            if sum([len(re.findall(r'#(URL|HASHTAG|USER)#\\w', tweet)) for tweet in user]) > 0:\n",
    "                print(user[0],label)\n",
    "                print([re.findall(r'#(URL|HASHTAG|USER)#\\w', tweet) for tweet in user])\n",
    "                break\n",
    "            out[label] += sum([len(re.findall(r'#(URL|HASHTAG|USER)#', tweet)) for tweet in user])\n",
    "    return out\n",
    "\n",
    "# count_max_words_alt_(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "env_btp",
   "language": "python",
   "name": "env_btp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
